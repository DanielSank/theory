%%%SECTION Linear Transformations%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Transformations}

In finite dimensional spaces all linear operators are of a single breed; they are all represented by matrix multiplication.  In our space $L^2$, however, there are essentially two kinds of linear operators, differential and integral.  In this chapter we are going to deal with differential operators because they are the ones that appear in our oscillator problem.  We deal with integral operators in the next chapter.  We already showed in chapter 1 that taking the derivative of a function is a linear transformation because \mbox{$\frac{d}{dx}(f+g) = \frac{df}{dx}+\frac{dg}{dx}$} etc.  The fact that differentiation is a linear transformation allows us a bit of insight into our oscillator problem.

\begin{flushleft} $\clubsuit$ \end{flushleft}
We have gotten as far as writing the equation of motion for the damped, driven oscillator in the form \mbox{$Z\ket{\Psi} = \ket{G}$} in equation (\ref{eq:AbstractForDamped}).  Although we did not say so before, the operator $Z$ is linear, as we now explain.  The first two terms of $Z$ are linear because they're just derivatives (squaring a linear operator gives a linear operator so the second derivative is linear) and the third term is multiplication by the scalar $\omega_0^2$, which is linear.  Since $Z$ is a sum of linear operators it is a linear operator.\footnote{By the way, the set of linear operators on $L^2$ over the complex numbers is itself a vector space.}  This means that (\ref{eq:AbstractForDamped}) is a linear transformation equation, just like (\ref{eq:abstractmotion}).  This similarity suggests that to solve the damped, driven oscillator problem we should use the same mathematical tools that we used in chapter 1 to solve the coupled oscillator.  In the case of the coupled oscillator we found that the problem was significantly simplified by working in a basis composed of eigenvectors of the linear transformation involved in the problem.  This suggests that to solve the damped, driven oscillator we should look for a basis composed of eigenfunctions of the operator $Z$.  $Z$ involves derivatives and scalar multiplication by $\omega _0^2$.  Since any vector is trivially an eigenvector of the operation of scalar multiplication we really only have to find vectors that are eigenvectors of the derivative.  We want to find vectors $\ket{E_\omega}$ that satisfy $D_t\ket{E_\omega} = \omega \ket{E_\omega}$.  We've used a common convention here where we've labeled the eigenvectors of an operator by their eigenvalues, ie. the eigenvalue of $\ket{E_\omega}$ is $\omega$.
\begin{flushright} $\clubsuit$ \end{flushright}

%%%subsection Derivative as a Hermetian Operator%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Derivative as a Hermetian Operator}
As explained in the previous section we wish to find a basis for $L^2$ composed entirely of eigenvectors of the derivative.  In finite dimensional linear algebra we know that for any Hermetian (or symmetric, in the case of real vector spaces) transformation there is an orthonormal basis composed entirely of eigenvectors of that transformation.  This is also true in the space $L^2$.\footnote{The proof of this fact is not difficult, it is almost a copy of one of the proofs that can be used for the finite dimensional case.  It is, however, not short, and as such will not be included here.  The proof falls under the field of mathematics called Sturm-Liouville Theory.}  This means that if the derivative turns out to be Hermetian then we can be confident that our desired basis exists.  With this in mind we check to see whether or not the derivative is Hermitian.  Let $D_t$ denote the derivative with respect to $t$,
\begin{eqnarray*}
\braket{f}{D_tg} &\stackrel{?}{=}& \braket{D_tf}{g}\\
\int f^*(t)~D_tg(t)~dt &\stackrel{?}{=}& \int (D_tf)^*(t)~g(t)~dt\\
\int f^*(t)~D_tg(t)~dt &\neq& -\int  f^*(t)~D_tg(t)~dt
\end{eqnarray*}
where the right hand side of the third line comes from using integration by parts and throwing out the boundary term on the assumption that the functions must go to zero at infinity.  This calculation shows that $D_t$ is not Hermetian, it is off by a sign.  We can fix this problem with a slight modification.  It turns out the the operator $-iD_t$ is Hermetian as we now check,
\begin{eqnarray*}
\braket{f}{-iD_tg} &\stackrel{?}{=}& \braket{-iD_tf}{g}\\
\int f^*(t)(-iD_t)g(t)~dt &\stackrel{?}{=}& \int [-iD_tf(t)]^*g(t)~dt\\
\int f^*(t)(-iD_t)g(t)~dt &\stackrel{?}{=}& \int iD_tf^*(t)g(t)~dt\\
\int f^*(t)(-iD_t)g(t)~dt &\stackrel{?}{=}& \int -if^*(t) D_tg(t)~dt\\
\int f^*(t)(-iD_t)g(t)~dt &=& \int f^*(t)(-iD_t)g(t)~dt.\\
\end{eqnarray*}
The minus sign is missing from the third line because we took the complex conjugate of $-i$ from the second line.  The fourth line follows from the third by integration by parts, again throwing out the boundary term.

We've now shown that the operator $-iD_t$ is Hermetian.  As discussed previously we expect that the eigenvectors of this operator form a basis for $L^2$.  What are these eigenvectors? The set of functions $e^{\omega t}$ where $\omega$ is any real number is a good guess.  These functions are eigenvectors of $-iD_t$ with eigenvalue $-i\omega$ as we can see, $-iD_te^{\omega t} = -i \omega e^{\omega t}$.  However, we don't want to use these functions because they are not in $L^2$ as you can check.  A slight modification fixes the problem.  We instead use the functions $E_\omega$ defined by the equation $E_\omega(t) = e^{i\omega t}$ whose eigenvalues of the hermetian transformation $-iD_t$ are $\omega$ as you should check.  These functions are in $L^2$.

ADD COMMENT ON THE FACT THAT THESE FUNCTIONS DO NOT HAVE FINITE NORM.

We now have two different bases for $L^2$.  The first is the set of functions $\delta_t$ and the second is the set of functions $E_\omega$ defined by the equation $E_\omega(t) = e^{i\omega t}$.

Let's recap what we just did.  We found a linear transformation $-iD_t$ on the vector space $L^2$ and proved that it is Hermitian.  Since we knew that the eigenvectors of a Hermitian operator form a basis we knew that we could find a basis for $L^2$ by finding the eigenvectors of $-iD_t$.  We did this and found that they were the functions $E_\omega$ defined by the equation $E_\omega(t) = e^{i\omega t}$.  These new functions $E_\omega$ are our new basis for $L^2$.

%%%subsection Changing Basis%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Changing Basis}
This section will show us the fruits of all our labors.  We will show explicity how one changes from one basis to another and how this simplifies the derivative operation.  As we go along take note of the similarities between what we do here and what we already did in chapter 1.

Let's say we have a vector $f$ in $L^2$.  Suppose that, as usual, our knowledge of this vector comes in the form of the numbers $f(t)$ for all $t$.  Now let's say we want to know the components of $f$ in the other basis, the one consisting of the functions $E_\omega$.  How do we do this?  We do the usual thing, take the inner product of $f$ and $E_\omega$,
\begin{displaymath} \braket{E_\omega}{f} = \int (e^{i\omega t})^*~f(t)~dt = \int e^{-i\omega t} f(t)~dt. \end{displaymath}
This integral shows us explicitly how to find the components of the vector $f$ in the basis of vectors $E_\omega$ if we know its components in the basis of vectors $\delta_{t}$.  Recall that our motivation behind using the basis composed of $E_\omega$ was that we wanted to simplify the derivative operation.  Now we'll see that we've succeeded; the derivative $D_t$ has a particularly simple form in the new basis.

Before we continue it will pay off to name our new basis.  We already named the basis consisting of the vectors $\delta_{t}$ by the letter $T$.  Now we name the basis composed of the vectors $E_\omega$ by the letter $\Omega$.

In the basis $T$ the operator $D_t$ is really ugly.  As we know, it is given by the following expression
\begin{displaymath} \braket{\delta_{t_0}}{D_tf} = \frac{df}{dt}(t_0) = \lim_{h \to 0} \frac{f(t_0+h) - f(t_0)}{h}. \end{displaymath}
This expression involves subtraction, division and \emph{limits}.  It is hard to imagine more mathematically complex operations.  Now let's see what the components of $D_tf$ look like in basis $\Omega$.
\begin{displaymath} \braket{E_\omega}{D_tf} = i\braket{E_\omega}{-iD_tf} = i\braket{-iD_tE_\omega}{f} = i\braket{\omega E_\omega}{f} = i\omega \braket{E_\omega}{f} \end{displaymath}
\begin{displaymath} \textrm{\emph{so in the basis}}~E_\omega \quad \framebox{$D_tf \rightarrow i\omega f$.} \end{displaymath}
We inserted the factor of $-i$ so that we could move the operator from the ket to the bra (remember that $-iD_t$ is Hermitian, not $D_t$).  This calculation shows that in basis $\Omega$ the operator $D_t$ just multiplies by a factor of $i\omega$.  This is indeed much simpler than that horrible limit expression that we had in basis $T$, and it makes our oscillator problem embarrasingly easy to solve.

One little note before we solve our oscillator problem.  We had an equation that said
\begin{displaymath} \braket{E_\omega}{f} = \int e^{-i\omega t} f(t)~dt \end{displaymath}
which tells us how to find the components of $f$ in the basis $\Omega$ if we know the components $f(t)$.  This integral is called the \textbf{Fourier Transform} of the function $f$.  As we have just shown, the Fourier transform turns derivatives into multiplication.  This is the main power of the Fourier transform.

The next section will develop some of the important properties of the fourier transform and bring us to the solution of our oscillator problem.
